#!/usr/bin/env python3
#
# Copyright (c) 2024 Antti Kervinen <antti.kervinen@gmail.com>
#
# License (MIT):
#
# Permission is hereby granted, free of charge, to any person
# obtaining a copy of this software and associated documentation files
# (the "Software"), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge,
# publish, distribute, sublicense, and/or sell copies of the Software,
# and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

r"""csv-query [options] FILE...

Options:
  -h, --help
  -c, --columns                      output only column headers (field names)
  -C, --no-columns                   do not output column headers at all
  -d, --delimiter DELIMITER          set field separator, the default is ";"
  -D, --out-delimiter OUT_DELIMITER  set output field separator, the default
                                     is DELIMITER.
  -W, --out-width COLWIDTH           set output column width, special widths:
                                     ld (left-align dynamic)
                                     ldx (left-align dynamic except leftmost)
                                     rd (right-align dynamic)
                                     rdx (right-align dynamic except rightmost)
  -t, --type TYPE                    convert input field values into TYPE:
                                     "f"/"i"/"l" for float/int/list
                                     where possible. Avoids conversions in
                                     EXPRs.
  -X, --exec-file STMT               execute STMT when starting new file.
  -x, --exec-row STMT                execute STMT on each row before evaluating
                                     SQL-like select, where or group EXPR.
  --post-mortem                      Start debugger on error.

Options for SQL-like queries:
  -s, --select FIELDSPEC[<DELIMITER> FIELDSPEC...] output values from selected
                                     fields.
                                     FIELDSPEC ::= FIELD | [NAME] "=" EXPR
                                     FIELDs can contain wildcards.
                                     NAME is column name for EXPR value.
                                     If NAME is omitted, EXPR is used as NAME.
                                     The default is --select "*".
  -w, --where EXPR                   output only rows where EXPR is True.
  -g, --group [[NAME]=]EXPR          Group rows by the value of EXPR. Every
                                     separate value forms a new group of data.
                                     If NAME is given, value of EXPR will be
                                     included in output in column NAME. If only
                                     = is given without name, EXPR is the column.
                                     A field name in a select EXPR evaluates to
                                     an array of field's values in each group.

Options for post-processing query result:
  --columns-from field               Transform arrays in the field into
                                     columns in a new table.

Scalar functions in EXPR:
  i(field), f(field), s(field)       field value as integer, float or string.
  sw(n, value)                       sliding window, array of n latest values.

Array functions in EXPR, use in conjuction with --group or sw():
  I(field), F(field), S(field)       field as int, float and string array.
  SUM(field), COUNT(field)           sum and number of elements in field.
  AVG(field), SD(field), VAR(field)  average, standard deviation and variance.
  COV(f0, f1), CORR(f0, f1)          covariance and correlation of two fields.
  PCT(n, field), MIN(field), MAX(field)  the nth percentile, min and max.
  SORTED(field)                      field values sorted
  STRJOIN(delim, field)              join string fields with delim.
  UNIQ(field)                        unique values in field.
  NORMALP(field)                     P value for "field normally distributed".
  TTESTP(field1, field2)             P value for T-test of field1 and field2.
  KSTESTP(field1, field2)            P value for Kolmogorov-Smirnov two-sample
                                     test of field1 and field2.
  MWUTESTP(field1, field2)           P value for Mann-Whitney U test.

Variables in EXPR:
  _file                              input CSV filename.
  _line                              number of data line in input CSV.
  _d                                 line data in a dictionary, access to
                                     _d["field name with spaces"].

Examples:
  # Print names and ages of cats matching a --where condition
  csv-query -f my-cats.csv \
    -s name -s age \
    -w "i(age)>=4 and f(weight)>4.5 and name.startswith('Marshmal')"

  # Print average weight of cats by age groups
  csv-query --type float -f my-cats.csv \
    -s 'avg_weight=AVG(weight);name' -g 'age_group=age//5'

  # Print 20th, 50th and 80th age percentiles
  csv-query -tf cats.csv -s '=PCT(20, age);=PCT(50, age);=PCT(90, age)'

  # Print the maximum weight in a sliding window of previous 5 cats
  csv-query -tf cats.csv -s '=MAX(sw(5, weight))'
"""

import csv
import fnmatch
import getopt
import re
import statistics
import sys

nan = float("nan")

# error prints error message and exits process
def error(msg, exit_status=1):
    if msg:
        sys.stderr.write("csv-query: %s\n" % (msg,))
    if exit_status is not None:
        sys.exit(exit_status)

_sw_history = {} # name -> [window data values]
_sw_updated = {} # name -> row of last update
def sw_context(name, row):
    """returns sliding window function for window 'name' on a row"""
    if name not in _sw_history:
        _sw_history[name] = []
        _sw_updated[name] = -1
    def sw(size, value):
        history = _sw_history[name]
        if _sw_updated[name] < row:
            _sw_updated[name] = row
            history.append(value)
            if len(history) > size:
                history.pop(0)
        return history[:]
    return sw

def query(data, fields, filt, group, exec_file=[], exec_row=[], query_env={}):
    if len(data) == 0:
        return []
    data_fields = set(data[0].keys())
    result = []
    aggr_funcs = {
        "F": lambda v: [float(f) for f in v],
        "I": lambda v: [int(i) for i in v],
        "S": lambda v: [str(s) for s in v],
        "AVG": lambda v: sum(v)/len(v) if len(v)>0 else nan,
        "SUM": lambda v: sum(v),
        "COUNT": lambda v: len(v),
        "PCT": lambda n, v: sorted(v)[int(len(v)*n/100) if n<100 else -1] if len(v)>0 else nan,
        "MIN": lambda v: min(v) if len(v)>0 else nan,
        "MAX": lambda v: max(v) if len(v)>0 else nan,
        "VAR": lambda v: statistics.variance(v) if len(v)>1 else nan,
        "SD": lambda v: statistics.stdev(v) if len(v)>1 else nan,
        "COV": lambda v0, v1: statistics.covariance(v0, v1) if len(v0)>1 else nan,
        "CORR": lambda v0, v1: statistics.correlation(v0, v1) if len(v0)>1 else nan,
        "ROUND": lambda v: [round(e) for e in v],
        "SORTED": lambda v: sorted(v),
        "STRJOIN": lambda delim, v: delim.join([str(e) for e in v]),
        "UNIQ": lambda v: sorted(set(v)),
    }
    scipy_aggr_funcs = {
        "NORMALP": lambda v0: scipy.stats.normaltest(v0)[1],
        "TTESTP": lambda v0, v1: scipy.stats.ttest_ind(v0, v1)[1],
        "KSTESTP": lambda v0, v1: scipy.stats.ks_2samp(v0, v1)[1],
        "MWUTESTP": lambda v0, v1: scipy.stats.mannwhitneyu(v0, v1)[1],
    }
    aggr_funcs.update(scipy_aggr_funcs)
    aggr_func_names = set(aggr_funcs.keys())
    scalar_funcs = {
        "f": lambda v: float(v),
        "i": lambda v: int(v),
        "s": lambda v: str(v),
    }
    typeid_castf = dict(aggr_funcs)
    typeid_castf.update(scalar_funcs)
    typeid_castf.update(query_env)
    typed_data_init = {key:{} for key in typeid_castf}
    filtered_data = []
    groupid_resultindex = {}

    field_exprs = []
    field_funcs = []
    field_names = []
    field_idx = []
    field_aggr = []
    field_sw = [] # field uses a sliding window
    extra_fields = []

    need_scipy = set()
    scipy_func_names = set(scipy_aggr_funcs.keys())
    for i, field in enumerate(fields):
        if isinstance(field, tuple): # field is (name, expr)
            ff = eval("lambda:" + field[1])
            field_exprs.append(field[1])
            field_funcs.append(ff)
            field_names.append(field[0])
            field_idx.append(i)
            names_in_code = set(ff.__code__.co_names)
            field_aggr.append(len(names_in_code.intersection(aggr_func_names)) > 0)
            need_scipy = need_scipy.union(scipy_func_names.intersection(names_in_code))
            field_sw.append("sw" in names_in_code)
            fields[i] = field[0] # replace (name, expr) with name only
    group_id_field = None
    if sum(field_aggr) > 0 and sum(field_sw) == 0 and group is None:
        # there are array functions but no sliding window or grouping
        # => put all rows to the same group.
        group = ("", "0")

    if group:
        group_id_field = group[0] if group[0] else "groupid"

    if group is not None:
        field_exprs.append(group[1])
        field_funcs.append(eval("lambda:" + group[1]))
        field_names.append(group_id_field)
        field_idx.append(len(fields))
        field_aggr.append(False)
        field_sw.append(False)
    extra_fields = sorted(
        set([n
             for ff in field_funcs
             for n in ff.__code__.co_names
             if n in data_fields and n not in fields
             ]))
    if group is not None:
        if group_id_field != "groupid":
            fields.append(group_id_field)
        else:
            extra_fields.append(group_id_field)

    if need_scipy:
        try:
            import scipy
            import scipy.stats
            typeid_castf["scipy"] = scipy
        except Exception as e:
            raise Exception("using %s requires scipy, but import scipy failed: %s" %
                            (", ".join(sorted(need_scipy)), e))
    filtered_data = []
    env = dict(typeid_castf)
    for stmt in exec_file:
        exec(stmt, env)
    for row_idx, row in enumerate(data):
        env['_d'] = row
        env['_line'] = row_idx+1
        env.update(row)
        for stmt in exec_row:
            exec(stmt, env)
        for i, ff in enumerate(field_funcs):
            if field_aggr[i] and group is not None: # aggregate functions will be evaluated after grouping
                continue
            if field_sw[i]:
                env["sw"] = sw_context(field_names[i], row_idx)
            field_value = eval(ff.__code__, env)
            env[field_names[i]] = field_value
            row[field_names[i]] = field_value
        if filt is None or eval(filt.__code__, env):
            filtered_data.append(row)

    user_and_extra_fields = fields + extra_fields
    for row in filtered_data:
        if group is None:
            result.append([row[field] for field in fields])
        else:
            group_value = row[group_id_field]
            resultindex = groupid_resultindex.get(group_value, None)
            if resultindex is None:
                resultindex = len(result)
                groupid_resultindex[group_value] = resultindex
                result.append([[] for _ in range(len(user_and_extra_fields))])
            for fieldindex, field in enumerate(user_and_extra_fields):
                if field == group_id_field:
                    result[resultindex][fieldindex] = row[field]
                elif field in row: # named aggregate fields are not yet in row
                    result[resultindex][fieldindex].append(row[field])

    if group is not None:
        aggregated_result = []
        env = dict(typeid_castf)
        for resrow in result:
            aggregated_result.append([resrow[i] for i in range(len(fields))])
            env.update({field: resrow[i] for i, field in enumerate(user_and_extra_fields)})
            try:
                for i, ff in enumerate(field_funcs):
                    if field_idx[i] < len(fields):
                        if field_aggr[i]:
                            aggregated_result[-1][field_idx[i]] = eval(ff.__code__, env)
            except Exception as e:
                error("group aggregation error on field %s, code %s: %s" %
                      (field_names[i], field_exprs[i], e))
            result = aggregated_result

    return result

def str_to_list(s):
    ss = s.strip()
    if not ss.startswith("["):
        raise ValueError("strings does not start with '['")
    return eval(ss)

def columns_from(colspecs, fields, result):
    field_idx = {} # {field: index in fields}
    new_fields = []
    new_field_idx = {} # {field: index in new_field}
    new_result = []
    for fidx, field in enumerate(fields):
        field_idx[field] = fidx
    max_row_idx = -1
    for field in colspecs:
        if not field in fields:
            raise Exception("invalid field '%s'" % (field,))
        for row_idx, row in enumerate(result):
            new_col_data = row[field_idx[field]]
            new_field = field + str(row_idx+1)
            new_field_idx[new_field] = len(new_fields)
            new_fields.append(new_field)
            for new_row_idx, new_data in enumerate(new_col_data):
                if len(new_result) <= new_row_idx:
                    new_result.append([])
                if len(new_result[new_row_idx]) < new_field_idx[new_field]:
                    new_result[new_row_idx].extend(["" for _ in range(new_field_idx[new_field] - len(new_result[new_row_idx]))])
                new_result[new_row_idx].append(new_data)
        if row_idx > max_row_idx:
            max_row_idx = row_idx
    for new_row_idx, new_row in enumerate(new_result):
        new_row.extend(["" for _ in range(len(new_fields) - len(new_row))])
    return new_fields, new_result

if __name__ == "__main__":
    import getopt
    opt_columns = False
    opt_no_columns = False
    opt_files = []
    opt_delimiter = ";"
    opt_out_delimiter = None
    opt_out_width = None
    opt_select = []
    opt_where = None
    opt_group = None
    opt_type = []
    opt_exec_file = []
    opt_exec_row = []
    opt_columns_from = []
    opt_post_mortem = False
    try:
        opts, remainder = getopt.gnu_getopt(
            sys.argv[1:], 'hcCd:D:W:f:X:x:s:w:g:t:',
            ['help', 'columns', 'no-columns', 'delimiter=', 'out-delimiter=',
             'out-width=', 'type=', 'exec-file=', 'exec-row=',
             'file=', 'select=', 'where=', 'group=',
             'columns-from=',
             'post-mortem'])
    except Exception as e:
        error(str(e))
    for opt, arg in opts:
        if opt in ["-h", "--help"]:
            print(__doc__)
            sys.exit(0)
        elif opt in ["-c", "--columns"]:
            opt_columns = True
        elif opt in ["-C", "--no-columns"]:
            opt_no_columns = True
        elif opt in ["-f", "--file"]:
            opt_files.append(arg)
        elif opt in ["-d", "--delimiter"]:
            opt_delimiter = arg
        elif opt in ["-D", "--out-delimiter"]:
            opt_out_delimiter = arg
        elif opt in ["-W", "--out-width"]:
            if arg in ["ld", "ldx", "rd", "rdx"]:
                opt_out_width = arg
            else:
                opt_out_width = int(arg)
        elif opt in ["-X", "--exec-file"]:
            opt_exec_file.append(compile(arg, "/parameter --exec-file='%s'" % (arg,), "single"))
        elif opt in ["-x", "--exec-row"]:
            opt_exec_row.append(compile(arg, "/parameter --exec-row='%s'" % (arg,), "single"))
        elif opt in ["-s", "--select"]:
            opt_select.extend(arg.split(opt_delimiter))
        elif opt in ["-w", "--where"]:
            if opt_where is not None:
                error("multiple -w/--where expressions")
            opt_where = eval("lambda:" + arg)
        elif opt in ["-g", "--group"]:
            if opt_group is not None:
                error("multiple -g/--group expressions")
            m = re.match('([a-zA-Z0-9_]*)=(?!=)(.*)', arg)
            if m:
                field, expr = m.groups()
                if field == "":
                    field = expr
            else:
                field, expr = "", arg
            opt_group = (field, expr)
        elif opt in ["-t", "--type"]:
            try:
                opt_type.append(
                    {"f": float, "float": float,
                     "i": int, "int": int,
                     "l": str_to_list, "list": str_to_list}[arg.lower()])
            except KeyError:
                error("invalid -t/--type '%s', expected 'f', 'i'" % (arg,))
        elif opt in ["--columns-from"]:
            opt_columns_from.extend(arg.split(opt_delimiter))
        elif opt in ["--post-mortem"]:
            opt_post_mortem = True
        else:
            error("internal error: unhandled option '%s'" % (opt,))

    if opt_out_delimiter is None:
        opt_out_delimiter = opt_delimiter

    opt_files.extend(remainder)
    if len(opt_files) == 0:
        opt_files.append("-")

    if opt_where is not None and len(opt_select) == 0:
        opt_select.append("*")
    if opt_where is None and opt_group is not None:
        opt_where = lambda: True

    for fname in opt_files:
        if fname == "-":
            data_reader = csv.DictReader(sys.stdin, delimiter=opt_delimiter)
        else:
            data_reader = csv.DictReader(open(fname), delimiter=opt_delimiter)
        if opt_columns:
            print(opt_delimiter.join(col for col in data_reader.fieldnames))
        if opt_select:
            data = [row for row in data_reader]
            if opt_type:
                # format data in rows
                for row in data:
                    for field in row:
                        for converter in opt_type:
                            try:
                                row[field] = converter(row[field])
                                break
                            except ValueError:
                                pass
            fields = []
            for field_pattern in opt_select:
                m = re.match('([a-zA-Z0-9_]*)=(?!=)(.*)', field_pattern)
                if m: # expression field
                    field, expr = m.groups()
                    if field == "":
                        field = expr
                    fields.append((field, expr))
                else:
                    fields.extend([f for f in data_reader.fieldnames if fnmatch.fnmatch(f, field_pattern) and f.strip()])
            try:
                result = query(data, fields, opt_where, opt_group,
                               exec_file=opt_exec_file,
                               exec_row=opt_exec_row,
                               query_env={'_file': fname})
            except:
                if opt_post_mortem:
                    import pdb
                    _, _, tb = sys.exc_info()
                    pdb.post_mortem(tb)
                else:
                    raise
            if opt_columns_from:
                fields, result = columns_from(opt_columns_from, fields, result)
            try:
                if opt_out_width is not None:
                    if opt_out_width in ["ld", "ldx", "rd", "rdx"]:
                        # output width per column
                        col_widths = [len(field) for field in fields]
                        for row in result:
                            for col_idx, col in enumerate(row):
                                if len(str(col)) > col_widths[col_idx]:
                                    col_widths[col_idx] = len(str(col))
                        if opt_out_width in ["rd", "rdx"]:
                            align = 1
                        else:
                            align = -1
                        col_fmt = ["%" + str(align * (col_width+2)) + "s" for col_width in col_widths]
                        if opt_out_width == "rdx":
                            col_fmt[-1] = "%s"
                        if opt_out_width == "ldx":
                            col_fmt[0] = "%" + str(col_widths[0]+2) + "s"
                    else:
                        # common output width to all columns
                        col_fmt = ["%" + str(opt_out_width) + "s" for _ in fields]
                    fields = [col_fmt[i] % (s,) for i, s in enumerate(fields)]
                    for row_idx, row in enumerate(result):
                        result[row_idx] = [col_fmt[i] % (col,) for i, col in enumerate(row)]
                if not opt_no_columns:
                    print(opt_out_delimiter.join(fields))
                for row in result:
                    print(opt_out_delimiter.join([str(col) for col in row]))
            except Exception as e:
                error(str(e), 1)
